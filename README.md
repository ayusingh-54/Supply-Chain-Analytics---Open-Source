<div align="center">

# ğŸ­ Supply Chain Analytics Platform

**Open-Source, AI-Ready Supply Chain Intelligence System**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109-009688.svg)](https://fastapi.tiangolo.com)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.31-FF4B4B.svg)](https://streamlit.io)
[![DuckDB](https://img.shields.io/badge/DuckDB-0.10-FFF000.svg)](https://duckdb.org)
[![MCP](https://img.shields.io/badge/MCP-1.2-purple.svg)](https://modelcontextprotocol.io)
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED.svg)](https://www.docker.com)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md)

> **Clone â†’ Upload Data â†’ Connect AI â†’ Get Insights**

A production-ready, open-source supply chain analytics platform featuring a **Streamlit UI** for data management, **DuckDB** for high-performance OLAP analytics, **FalkorDB** for graph-based relationship analysis, and a **Model Context Protocol (MCP) Server** that enables seamless integration with **Claude Desktop**, **Claude Code**, **Cursor**, and any MCP-compatible AI client.

[Quick Start](#-quick-start) Â· [Architecture](#-architecture) Â· [Documentation](docs/) Â· [API Reference](#-api-reference) Â· [Contributing](CONTRIBUTING.md)

</div>

---

## Table of Contents

- [Features](#-features)
- [Quick Start](#-quick-start)
- [Architecture](#-architecture)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Usage Guide](#-usage-guide)
- [Data Categories & Schemas](#-data-categories--schemas)
- [API Reference](#-api-reference)
- [MCP Server & AI Integration](#-mcp-server--ai-integration)
- [Graph Database (FalkorDB)](#-graph-database-falkordb)
- [Docker Deployment](#-docker-deployment)
- [Testing](#-testing)
- [Project Structure](#-project-structure)
- [Roadmap](#-roadmap)
- [Contributing](#-contributing)
- [License](#-license)
- [Acknowledgments](#-acknowledgments)

---

## âœ¨ Features

### Core Platform

- **ğŸ“¤ Drag-and-Drop File Upload** â€” Upload CSV/XLSX files with automatic schema validation
- **ğŸ” Data Quality Engine** â€” Automated duplicate detection, null handling, constraint validation, and quality scoring
- **ğŸ“Š Real-Time Dashboard** â€” Monitor file status, data quality, KPIs, and inventory alerts
- **ğŸ—‚ï¸ File Versioning** â€” Full version history with archive/restore capabilities
- **ğŸ“¥ Template Downloads** â€” Pre-built CSV/XLSX templates for every data category

### Analytics & Intelligence

- **ğŸ“ˆ KPI Dashboard** â€” Revenue, inventory turnover, supplier ratings, order volumes
- **ğŸ”” Anomaly Detection** â€” Statistical outlier detection across sales, inventory, and suppliers
- **ğŸ“‰ Demand Forecasting** â€” Moving-average based demand prediction per SKU
- **âš ï¸ Supplier Risk Analysis** â€” Multi-factor risk scoring (lead time, rating, concentration)
- **ğŸ›’ Smart Reorder Recommendations** â€” Priority-ranked reorder suggestions based on velocity and lead times
- **ğŸ•¸ï¸ Supply Chain Graph** â€” Relationship mapping between suppliers, products, and purchase orders

### AI Integration (MCP)

- **12 MCP Tools** â€” Purpose-built tools that give AI assistants direct access to your supply chain data
- **Multi-Client Support** â€” Works with Claude Desktop, Claude Code, Cursor, and any MCP-compatible client
- **Dual Transport** â€” stdio (default) and SSE transport modes
- **Auto-Config Generation** â€” One-click JSON config generation for every supported AI client
- **Read-Only by Default** â€” SQL injection protection and query whitelisting for production safety

### Infrastructure

- **âš¡ DuckDB** â€” Embedded OLAP database for sub-second analytical queries on millions of rows
- **ğŸ•¸ï¸ FalkorDB** â€” Optional graph database for supply chain relationship modeling (Cypher queries)
- **ğŸ³ Docker Compose** â€” Full stack orchestration with persistent volumes
- **ğŸ§ª Test Suite** â€” pytest-based unit and integration tests
- **ğŸ”§ One-Command Setup** â€” Platform-specific start scripts for Windows (`start.bat`) and Linux/macOS (`start.sh`)

---

## ğŸš€ Quick Start

### Prerequisites

| Requirement                 | Version | Notes                        |
| --------------------------- | ------- | ---------------------------- |
| Python                      | 3.11+   | Core runtime                 |
| pip                         | Latest  | Package management           |
| Docker _(optional)_         | 20.10+  | For containerized deployment |
| Docker Compose _(optional)_ | 2.0+    | For full-stack orchestration |

### Option A: Local Development (Recommended)

```bash
# 1. Clone the repository
git clone https://github.com/your-org/supply-chain-analytics.git
cd supply-chain-analytics

# 2. Create and activate virtual environment
python -m venv venv
# Windows:
venv\Scripts\activate
# Linux/macOS:
source venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Initialize the database
python scripts/setup_database.py

# 5. (Optional) Generate sample data
python scripts/generate_sample_data.py

# 6. Start the platform
# Terminal 1 â€” Backend API
cd backend && uvicorn main:app --reload --port 8000

# Terminal 2 â€” Streamlit Frontend
cd streamlit_app && streamlit run app.py

# Terminal 3 â€” MCP Server (for AI integration)
cd mcp_server && python server.py
```

### Option B: One-Command Start

```bash
# Windows
start.bat

# Linux / macOS
chmod +x start.sh && ./start.sh
```

### Option C: Docker Compose

```bash
docker-compose up --build
```

### Verify Installation

| Service          | URL                          | Description                                |
| ---------------- | ---------------------------- | ------------------------------------------ |
| **Streamlit UI** | http://localhost:8501        | Data upload & dashboard                    |
| **FastAPI Docs** | http://localhost:8000/docs   | Interactive API documentation (Swagger UI) |
| **API ReDoc**    | http://localhost:8000/redoc  | Alternative API docs                       |
| **Health Check** | http://localhost:8000/health | Service health status                      |

---

## ğŸ“‹ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          PRESENTATION LAYER                              â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    Streamlit Frontend (:8501)                      â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚Dashboard â”‚ â”‚ Upload   â”‚ â”‚ History  â”‚ â”‚MCP Configâ”‚ â”‚Settingsâ”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                               â”‚ HTTP/REST                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          SERVICE LAYER                                   â”‚
â”‚                               â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    FastAPI Backend (:8000)                          â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚ /api/files   â”‚ â”‚/api/database â”‚ â”‚/api/templatesâ”‚ â”‚ /api/mcp â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚         â”‚                â”‚                                         â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚              Services Layer                                  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ FileService  â”‚ â”‚DuckDBService â”‚ â”‚ FalkorDBService      â”‚â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ â€¢ Validation â”‚ â”‚ â€¢ Analytics  â”‚ â”‚ â€¢ Graph Queries      â”‚â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ â€¢ Quality    â”‚ â”‚ â€¢ KPIs       â”‚ â”‚ â€¢ Relationship Sync  â”‚â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ â€¢ Storage    â”‚ â”‚ â€¢ SQL Engine â”‚ â”‚ â€¢ Cypher Engine      â”‚â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                â”‚                    â”‚                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          DATA LAYER                  â”‚                    â”‚
â”‚                                â”‚                    â”‚                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚       DuckDB (Embedded)        â”‚  â”‚     FalkorDB (:6379)           â”‚ â”‚
â”‚  â”‚  â€¢ sales_data                  â”‚  â”‚  â€¢ :Supplier nodes             â”‚ â”‚
â”‚  â”‚  â€¢ inventory_data              â”‚  â”‚  â€¢ :Product nodes              â”‚ â”‚
â”‚  â”‚  â€¢ supplier_data               â”‚  â”‚  â€¢ :PurchaseOrder nodes        â”‚ â”‚
â”‚  â”‚  â€¢ purchase_order_data         â”‚  â”‚  â€¢ :SUPPLIES relationships     â”‚ â”‚
â”‚  â”‚  â€¢ file_uploads (metadata)     â”‚  â”‚  â€¢ :ORDERS relationships       â”‚ â”‚
â”‚  â”‚  â€¢ file_versions               â”‚  â”‚  â€¢ :FROM_SUPPLIER relationshipsâ”‚ â”‚
â”‚  â”‚  â€¢ data_quality_issues         â”‚  â”‚                                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                       AI INTEGRATION LAYER                               â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                MCP Server (stdio / SSE :3001)                      â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚ Data Queries   â”‚ â”‚ Analytics      â”‚ â”‚ Intelligence           â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ sales        â”‚ â”‚ â€¢ KPI dashboardâ”‚ â”‚ â€¢ anomaly detection    â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ inventory    â”‚ â”‚ â€¢ data quality â”‚ â”‚ â€¢ demand forecasting   â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ suppliers    â”‚ â”‚ â€¢ supply graph â”‚ â”‚ â€¢ supplier risk        â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ POs          â”‚ â”‚ â€¢ custom SQL   â”‚ â”‚ â€¢ reorder suggestions  â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚             â”‚                  â”‚                      â”‚                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              MCP-Compatible AI Clients                            â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚
â”‚  â”‚  â”‚Claude Desktopâ”‚ â”‚ Claude Code  â”‚ â”‚   Cursor     â”‚  + Others    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

| Layer            | Technology                | Purpose                                     |
| ---------------- | ------------------------- | ------------------------------------------- |
| Frontend         | Streamlit 1.31            | Interactive data management UI              |
| Backend API      | FastAPI 0.109 + Uvicorn   | High-performance async REST API             |
| Analytics DB     | DuckDB 0.10               | Embedded columnar OLAP engine               |
| Graph DB         | FalkorDB 1.0 _(optional)_ | Supply chain relationship modeling          |
| AI Protocol      | MCP 1.2                   | Model Context Protocol for AI tool exposure |
| Data Processing  | Pandas 2.1 + NumPy 1.26   | DataFrame operations and validation         |
| Visualization    | Plotly 5.18               | Interactive charts and plots                |
| Containerization | Docker + Compose          | Production deployment                       |

---

## ğŸ› ï¸ Installation

### System Requirements

| Component | Minimum                            | Recommended                |
| --------- | ---------------------------------- | -------------------------- |
| CPU       | 2 cores                            | 4+ cores                   |
| RAM       | 2 GB                               | 8+ GB                      |
| Disk      | 500 MB                             | 5+ GB (for large datasets) |
| OS        | Windows 10, Ubuntu 20.04, macOS 12 | Latest LTS versions        |

### Step-by-Step Installation

#### 1. Clone Repository

```bash
git clone https://github.com/your-org/supply-chain-analytics.git
cd supply-chain-analytics
```

#### 2. Create Virtual Environment

```bash
python -m venv venv

# Activate
# Windows:
venv\Scripts\activate
# Linux/macOS:
source venv/bin/activate
```

#### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

<details>
<summary><strong>ğŸ“¦ Full Dependency List</strong></summary>

| Package             | Version | Purpose                         |
| ------------------- | ------- | ------------------------------- |
| `fastapi`           | 0.109.0 | REST API framework              |
| `uvicorn`           | 0.27.0  | ASGI server                     |
| `python-multipart`  | 0.0.6   | File upload handling            |
| `streamlit`         | 1.31.0  | Web frontend framework          |
| `plotly`            | 5.18.0  | Interactive visualizations      |
| `pandas`            | 2.1.4   | Data manipulation               |
| `openpyxl`          | 3.1.2   | Excel file support              |
| `duckdb`            | 0.10.0  | Analytical database             |
| `falkordb`          | 1.0.3   | Graph database client           |
| `redis`             | 5.0.1   | FalkorDB connection layer       |
| `requests`          | 2.31.0  | HTTP client                     |
| `httpx`             | 0.26.0  | Async HTTP client               |
| `mcp`               | 1.2.0   | Model Context Protocol SDK      |
| `python-dotenv`     | 1.0.0   | Environment configuration       |
| `pydantic`          | 2.5.3   | Data validation & serialization |
| `pydantic-settings` | 2.1.0   | Settings management             |
| `numpy`             | 1.26.4  | Numerical operations            |

</details>

#### 4. Initialize Database

```bash
python scripts/setup_database.py
```

#### 5. (Optional) Generate Sample Data

```bash
python scripts/generate_sample_data.py
```

This generates 4 CSV files in `sample_data/`:

- `sales_data.csv` â€” 500 sales transactions
- `inventory_data.csv` â€” 50 SKU inventory records
- `supplier_data.csv` â€” 20 supplier profiles
- `purchase_order_data.csv` â€” 200 purchase orders

#### 6. (Optional) Start FalkorDB for Graph Features

```bash
docker run -d --name falkordb -p 6379:6379 falkordb/falkordb:latest
```

---

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file in the project root (or copy from `.env.example`):

```env
# Database
DUCKDB_PATH=./data/supply_chain.duckdb

# Storage
STORAGE_PATH=./storage

# FalkorDB (optional)
USE_FALKORDB=false
FALKORDB_HOST=localhost
FALKORDB_PORT=6379

# Upload Limits
MAX_FILE_SIZE_MB=200

# MCP Server
MCP_SERVER_PORT=3001
```

### Configuration Reference

| Variable           | Default                      | Description                     |
| ------------------ | ---------------------------- | ------------------------------- |
| `DUCKDB_PATH`      | `./data/supply_chain.duckdb` | Path to DuckDB database file    |
| `STORAGE_PATH`     | `./storage`                  | Root directory for file storage |
| `USE_FALKORDB`     | `false`                      | Enable FalkorDB graph database  |
| `FALKORDB_HOST`    | `localhost`                  | FalkorDB server hostname        |
| `FALKORDB_PORT`    | `6379`                       | FalkorDB server port            |
| `MAX_FILE_SIZE_MB` | `200`                        | Maximum upload file size in MB  |
| `MCP_SERVER_PORT`  | `3001`                       | Port for SSE transport mode     |

---

## ğŸ“– Usage Guide

### 1. Upload Data

Navigate to **ğŸ“¤ Upload Files** in the Streamlit UI:

1. **Select category** (Sales, Inventory, Supplier, or Purchase Order)
2. **Download template** to see the expected format
3. **Upload your CSV/XLSX** file
4. **Review validation** â€” schema checks, quality issues, data preview
5. **Confirm upload** â€” data is loaded into DuckDB

### 2. Monitor Dashboard

The **ğŸ“Š Dashboard** shows:

- Upload status for all 4 data categories
- Row counts and data quality scores
- Quick actions: preview data, replace files, view history

### 3. Connect AI

Go to **ğŸ”— MCP Config** and copy the generated JSON config into your AI client:

**Claude Desktop:**

```
Windows: %APPDATA%\Claude\claude_desktop_config.json
macOS:   ~/Library/Application Support/Claude/claude_desktop_config.json
```

**Claude Code:**

```
.claude/mcp.json (in your project root)
```

**Cursor:**

```
.cursor/mcp.json (in your project root)
```

### 4. Ask AI Questions

Once connected, ask questions like:

- _"What are my top 10 selling products by revenue?"_
- _"Which inventory items are below their reorder point?"_
- _"Analyze supplier risk for SUP-003"_
- _"Forecast demand for SKU-001 over the next 30 days"_
- _"Show me the supply chain network overview"_
- _"Run a query: SELECT sku, SUM(revenue) FROM sales_data GROUP BY sku ORDER BY 2 DESC LIMIT 10"_

---

## ğŸ“ Data Categories & Schemas

### Sales Data

| Column          | Type    | Required | Constraints                         |
| --------------- | ------- | -------- | ----------------------------------- |
| `date`          | DATE    | âœ…       | Valid date, not in future (warning) |
| `sku`           | VARCHAR | âœ…       | â€”                                   |
| `quantity`      | FLOAT   | âœ…       | â‰¥ 0                                 |
| `revenue`       | FLOAT   | âœ…       | â‰¥ 0                                 |
| `customer_name` | VARCHAR | âŒ       | â€”                                   |
| `region`        | VARCHAR | âŒ       | â€”                                   |
| `category`      | VARCHAR | âŒ       | â€”                                   |

### Inventory Data

| Column          | Type    | Required | Constraints            |
| --------------- | ------- | -------- | ---------------------- |
| `sku`           | VARCHAR | âœ…       | â€”                      |
| `qty_on_hand`   | INTEGER | âœ…       | â‰¥ 0                    |
| `reorder_point` | INTEGER | âœ…       | â‰¥ 0                    |
| `location`      | VARCHAR | âŒ       | â€”                      |
| `unit_cost`     | FLOAT   | âŒ       | â€”                      |
| `supplier_id`   | VARCHAR | âŒ       | Links to Supplier Data |

### Supplier Data

| Column          | Type    | Required | Constraints       |
| --------------- | ------- | -------- | ----------------- |
| `supplier_id`   | VARCHAR | âœ…       | Unique identifier |
| `supplier_name` | VARCHAR | âœ…       | â€”                 |
| `lead_time`     | INTEGER | âœ…       | â‰¥ 0 (days)        |
| `contact_email` | VARCHAR | âŒ       | â€”                 |
| `rating`        | FLOAT   | âŒ       | 0.0 â€“ 5.0         |
| `country`       | VARCHAR | âŒ       | â€”                 |

### Purchase Order Data

| Column          | Type    | Required | Constraints            |
| --------------- | ------- | -------- | ---------------------- |
| `po_number`     | VARCHAR | âœ…       | Unique identifier      |
| `sku`           | VARCHAR | âœ…       | Links to Inventory     |
| `quantity`      | FLOAT   | âœ…       | â‰¥ 0                    |
| `order_date`    | DATE    | âŒ       | â€”                      |
| `delivery_date` | DATE    | âŒ       | â€”                      |
| `supplier_id`   | VARCHAR | âŒ       | Links to Supplier Data |

### Data Quality Pipeline

The upload pipeline applies these checks automatically:

1. **Schema Validation** â€” Required columns present, types correct
2. **Duplicate Detection** â€” Exact duplicate rows removed (auto-resolved)
3. **Null Handling** â€” Rows with null required fields removed (auto-resolved)
4. **Constraint Checks** â€” Negative value validation (auto-resolved)
5. **Date Validation** â€” Future date detection (flagged as warning)
6. **Quality Scoring** â€” 0â€“100 score based on valid row ratio and unresolved issues

---

## ğŸ”Œ API Reference

### Base URL

```
http://localhost:8000
```

### Endpoints

#### Health & Info

| Method | Endpoint  | Description                 |
| ------ | --------- | --------------------------- |
| `GET`  | `/`       | API information             |
| `GET`  | `/health` | Health check with DB status |

#### File Management (`/api/files`)

| Method | Endpoint                        | Description                       |
| ------ | ------------------------------- | --------------------------------- |
| `POST` | `/api/files/upload`             | Upload and process a data file    |
| `POST` | `/api/files/validate`           | Validate a file without uploading |
| `GET`  | `/api/files/status`             | Get status of all file categories |
| `GET`  | `/api/files/status/{category}`  | Status for specific category      |
| `GET`  | `/api/files/history/{category}` | Version history for a category    |
| `GET`  | `/api/files/preview/{category}` | Preview uploaded data             |
| `GET`  | `/api/files/schema/{category}`  | Expected schema for category      |

#### Database Analytics (`/api/database`)

| Method | Endpoint                          | Description                      |
| ------ | --------------------------------- | -------------------------------- |
| `GET`  | `/api/database/kpis`              | Key performance indicators       |
| `GET`  | `/api/database/sales-summary`     | Sales summary with filters       |
| `GET`  | `/api/database/inventory-status`  | Inventory reorder alerts         |
| `GET`  | `/api/database/supplier-analysis` | Supplier performance analysis    |
| `POST` | `/api/database/query`             | Execute custom SQL (SELECT only) |
| `POST` | `/api/database/refresh`           | Sync data to FalkorDB graph      |

#### Templates (`/api/templates`)

| Method | Endpoint                             | Description                |
| ------ | ------------------------------------ | -------------------------- |
| `GET`  | `/api/templates/download/{category}` | Download template CSV/XLSX |

#### MCP Configuration (`/api/mcp`)

| Method | Endpoint                         | Description                     |
| ------ | -------------------------------- | ------------------------------- |
| `GET`  | `/api/mcp/config`                | Full MCP config for all clients |
| `GET`  | `/api/mcp/config/claude-desktop` | Claude Desktop config only      |
| `GET`  | `/api/mcp/config/cursor`         | Cursor config only              |

### Upload Example

```bash
curl -X POST http://localhost:8000/api/files/upload \
  -F "file=@sales_data.csv" \
  -F "file_category=sales" \
  -F "upload_mode=replace" \
  -F "uploaded_by=cli_user"
```

### Query Example

```bash
curl -X POST http://localhost:8000/api/database/query \
  -H "Content-Type: application/json" \
  -d '{"query": "SELECT sku, SUM(revenue) as total FROM sales_data GROUP BY sku ORDER BY total DESC LIMIT 5"}'
```

---

## ğŸ¤– MCP Server & AI Integration

### Overview

The MCP Server exposes 12 supply chain tools via the [Model Context Protocol](https://modelcontextprotocol.io), allowing AI assistants to query, analyze, and reason about your supply chain data in natural language.

### Transport Modes

```bash
# stdio (default) â€” for Claude Desktop / Claude Code / Cursor
python mcp_server/server.py

# SSE â€” for web-based or remote clients
python mcp_server/server.py --sse
```

### Available Tools

| #   | Tool                          | Description                             | Key Parameters                                           |
| --- | ----------------------------- | --------------------------------------- | -------------------------------------------------------- |
| 1   | `query_sales_data`            | Query sales with filters & aggregations | `start_date`, `end_date`, `sku`, `region`, `aggregation` |
| 2   | `query_inventory`             | Check stock levels & reorder status     | `sku`, `location`, `status_filter`                       |
| 3   | `query_suppliers`             | Supplier information & performance      | `supplier_id`, `country`, `min_rating`                   |
| 4   | `query_purchase_orders`       | Purchase order analysis                 | `po_number`, `sku`, `supplier_id`                        |
| 5   | `run_sql_query`               | Execute custom SQL (SELECT only)        | `query` (required)                                       |
| 6   | `get_data_quality_report`     | Data quality metrics for all files      | â€”                                                        |
| 7   | `get_kpi_dashboard`           | Key performance indicators              | â€”                                                        |
| 8   | `detect_anomalies`            | Find outliers & unusual patterns        | `category` (sales/inventory/supplier/all)                |
| 9   | `forecast_demand`             | Demand forecasting per SKU              | `sku` (required), `periods`                              |
| 10  | `analyze_supplier_risk`       | Multi-factor supplier risk scoring      | `supplier_id`                                            |
| 11  | `get_reorder_recommendations` | Smart reorder suggestions               | â€”                                                        |
| 12  | `get_supply_chain_graph`      | Supply chain relationship network       | `focus` (overview/supplier_products/product_orders)      |

### Security

- Only `SELECT` queries are permitted through `run_sql_query`
- Dangerous SQL keywords (`DROP`, `DELETE`, `INSERT`, `UPDATE`, `ALTER`, `CREATE`, `TRUNCATE`, `EXEC`) are blocked
- DuckDB connection is read-only in the MCP server
- No filesystem access is exposed to AI clients

---

## ğŸ•¸ï¸ Graph Database (FalkorDB)

FalkorDB provides a property graph model of your supply chain. When enabled, the platform syncs data from DuckDB into a graph with the following schema:

### Graph Model

```
(:Supplier) -[:SUPPLIES]-> (:Product)
(:PurchaseOrder) -[:ORDERS]-> (:Product)
(:PurchaseOrder) -[:FROM_SUPPLIER]-> (:Supplier)
```

### Node Properties

| Node              | Properties                                              |
| ----------------- | ------------------------------------------------------- |
| **Supplier**      | `supplier_id`, `name`, `lead_time`, `country`, `rating` |
| **Product**       | `sku`, `qty_on_hand`, `reorder_point`, `location`       |
| **PurchaseOrder** | `po_number`, `quantity`, `order_date`                   |

### Enabling FalkorDB

```bash
# Start FalkorDB
docker run -d --name falkordb -p 6379:6379 falkordb/falkordb:latest

# Set environment variable
USE_FALKORDB=true

# Sync data (via Settings page or API)
curl -X POST http://localhost:8000/api/database/refresh
```

---

## ğŸ³ Docker Deployment

### Full Stack (Recommended)

```bash
docker-compose up --build -d
```

This starts:

- **sc-backend** â€” FastAPI on port 8000
- **sc-streamlit** â€” Streamlit on port 8501
- **sc-falkordb** â€” FalkorDB on port 6379

### Individual Services

```bash
# Backend only
docker build -f backend/Dockerfile -t sc-backend .
docker run -p 8000:8000 -v ./data:/app/data sc-backend

# Streamlit only
docker build -f streamlit_app/Dockerfile -t sc-streamlit .
docker run -p 8501:8501 -e API_BASE_URL=http://host.docker.internal:8000/api sc-streamlit
```

### Docker Compose Configuration

```yaml
services:
  backend:
    ports: ["8000:8000"]
    volumes: [./data:/app/data, ./uploads:/app/uploads]
    environment:
      DUCKDB_PATH: /app/data/supply_chain.duckdb
      USE_FALKORDB: "true"
      FALKORDB_HOST: falkordb

  streamlit:
    ports: ["8501:8501"]
    environment:
      API_BASE_URL: http://backend:8000/api

  falkordb:
    image: falkordb/falkordb:latest
    ports: ["6379:6379"]
    volumes: [falkordb_data:/data]
```

---

## ğŸ§ª Testing

### Run Tests

```bash
# All tests
pytest tests/ -v

# Specific test modules
pytest tests/test_api.py -v        # API endpoint tests
pytest tests/test_services.py -v   # Service layer tests

# With coverage
pytest tests/ --cov=backend --cov-report=html
```

### Test Categories

| Test Module        | Coverage                                                                                               |
| ------------------ | ------------------------------------------------------------------------------------------------------ |
| `test_api.py`      | Health endpoint, template routes, database routes, MCP config, file status                             |
| `test_services.py` | File validation, schema checking, quality engine, DuckDB operations, MCP tools, sample data generation |

### Test Environment

Tests use:

- Temporary DuckDB database (auto-cleaned)
- FalkorDB disabled (`USE_FALKORDB=false`)
- Isolated storage directory

---

## ğŸ“‚ Project Structure

```
supply-chain-analytics/
â”œâ”€â”€ backend/                      # FastAPI Backend Application
â”‚   â”œâ”€â”€ main.py                   # Application entry point & CORS config
â”‚   â”œâ”€â”€ Dockerfile                # Backend container image
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â”œâ”€â”€ files.py          # File upload, validation, preview endpoints
â”‚   â”‚       â”œâ”€â”€ database.py       # Analytics, KPIs, custom query endpoints
â”‚   â”‚       â”œâ”€â”€ templates.py      # Template download endpoints
â”‚   â”‚       â””â”€â”€ mcp_config.py     # MCP configuration generator
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py             # Pydantic settings & schema rules
â”‚   â”‚   â””â”€â”€ database.py           # DuckDB connection & table initialization
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py            # Pydantic request/response models
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ duckdb_service.py     # DuckDB analytics & metadata operations
â”‚       â”œâ”€â”€ falkordb_service.py   # Graph database operations
â”‚       â””â”€â”€ file_service.py       # File validation, quality, upload pipeline
â”‚
â”œâ”€â”€ streamlit_app/                # Streamlit Frontend Application
â”‚   â”œâ”€â”€ app.py                    # Main app entry & system status
â”‚   â”œâ”€â”€ Dockerfile                # Frontend container image
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ 1_ğŸ“Š_Dashboard.py     # File status & data quality overview
â”‚   â”‚   â”œâ”€â”€ 2_ğŸ“¤_Upload_Files.py  # Multi-step upload wizard
â”‚   â”‚   â”œâ”€â”€ 3_ğŸ“œ_History.py       # File version management
â”‚   â”‚   â”œâ”€â”€ 4_ğŸ”—_MCP_Config.py   # AI client config generator
â”‚   â”‚   â””â”€â”€ 5_âš™ï¸_Settings.py     # Application settings & quick actions
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ api_client.py         # FastAPI HTTP client wrapper
â”‚
â”œâ”€â”€ mcp_server/                   # Model Context Protocol Server
â”‚   â”œâ”€â”€ server.py                 # MCP server with 12 tools (stdio + SSE)
â”‚   â””â”€â”€ tools/                    # Tool extensions (future)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_database.py         # Database initialization script
â”‚   â””â”€â”€ generate_sample_data.py   # Realistic sample data generator
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_api.py               # API integration tests
â”‚   â””â”€â”€ test_services.py          # Service unit tests
â”‚
â”œâ”€â”€ data/                         # DuckDB database files (git-ignored)
â”œâ”€â”€ storage/                      # File storage
â”‚   â”œâ”€â”€ uploads/
â”‚   â”‚   â”œâ”€â”€ active/               # Currently active files
â”‚   â”‚   â”œâ”€â”€ archive/              # Archived file versions
â”‚   â”‚   â””â”€â”€ staging/              # Temporary upload staging
â”‚   â”œâ”€â”€ rejected/                 # Files that failed validation
â”‚   â”œâ”€â”€ templates/                # Template files
â”‚   â””â”€â”€ error_reports/            # Error logs
â”œâ”€â”€ sample_data/                  # Generated sample CSV files
â”‚
â”œâ”€â”€ docker-compose.yml            # Full stack Docker orchestration
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ start.bat                     # Windows quick-start script
â”œâ”€â”€ start.sh                      # Linux/macOS quick-start script
â”œâ”€â”€ .env.example                  # Environment variable template
â”œâ”€â”€ LICENSE                       # MIT License
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ CONTRIBUTING.md               # Contribution guidelines
â”œâ”€â”€ CHANGELOG.md                  # Version history
â”œâ”€â”€ CODE_OF_CONDUCT.md            # Community standards
â”œâ”€â”€ SECURITY.md                   # Security policy
â”œâ”€â”€ Makefile                      # Common development commands
â””â”€â”€ pyproject.toml                # Python project metadata
```

---

## ğŸ—ºï¸ Roadmap

### v1.1 â€” Enhanced Analytics

- [ ] Time-series demand forecasting with Prophet/ARIMA integration
- [ ] ABC/XYZ inventory classification
- [ ] Supplier scorecards with trend analysis
- [ ] Custom dashboard builder

### v1.2 â€” Enterprise Features

- [ ] User authentication and role-based access control (RBAC)
- [ ] Audit trail for all data operations
- [ ] Scheduled data refresh (cron-based)
- [ ] Email/Slack alerting for anomalies and reorder triggers

### v1.3 â€” Advanced AI

- [ ] Multi-agent MCP workflows
- [ ] Natural language to SQL with validation
- [ ] AI-generated supply chain reports
- [ ] What-if scenario simulation

### v2.0 â€” Scale & Integration

- [ ] PostgreSQL/ClickHouse backend option
- [ ] Real-time data streaming (Kafka/Pulsar)
- [ ] ERP system connectors (SAP, Oracle, NetSuite)
- [ ] REST webhook support for event-driven architecture

---

## ğŸ¤ Contributing

We welcome contributions of all kinds! Please read our [Contributing Guide](CONTRIBUTING.md) for details on:

- How to submit bug reports and feature requests
- Development setup and coding standards
- Pull request process and review guidelines
- Code of Conduct

```bash
# Quick contribution setup
git clone https://github.com/your-org/supply-chain-analytics.git
cd supply-chain-analytics
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt
pytest tests/ -v  # Ensure tests pass
```

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€” see the [LICENSE](LICENSE) file for details.

You are free to use, modify, and distribute this software for any purpose, including commercial applications.

---

## ğŸ™ Acknowledgments

- [FastAPI](https://fastapi.tiangolo.com) â€” High-performance Python web framework
- [Streamlit](https://streamlit.io) â€” Rapid data app development
- [DuckDB](https://duckdb.org) â€” In-process analytical SQL engine
- [FalkorDB](https://www.falkordb.com) â€” Ultra-fast graph database
- [Model Context Protocol](https://modelcontextprotocol.io) â€” AI tool integration standard
- [Pydantic](https://docs.pydantic.dev) â€” Data validation using Python type hints
- [Pandas](https://pandas.pydata.org) â€” Data analysis and manipulation

---

<div align="center">

**Built with â¤ï¸ by the Open Source Community**

[â¬† Back to Top](#-supply-chain-analytics-platform)

</div>
